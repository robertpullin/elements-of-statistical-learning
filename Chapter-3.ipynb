{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 - Linear Methods for Regression\n",
    "\n",
    "## 3.1 - Introduction\n",
    "\n",
    "Linear regression models are simple and often provide an adequate and interpretable description of how the inputs affect the output. \n",
    "\n",
    "For prediction purposes they can sometimes outperform fancier nonlinear models, especially in situations with small numbers of training cases, low signal-to-noise ratio or sparse data. \n",
    "\n",
    "Finally, linear methods can be applied to transformations of the inputs and this considerably expands their scope. These generalizations are sometimes called basis-function methods.\n",
    "\n",
    "Many nonlinear techniques are direct generalizations of linear methods.\n",
    "\n",
    "## 3.2 - Linear Regression Models and Least Squares\n",
    "\n",
    "The linear regression model has the form\n",
    "$$ f(X) = \\beta_0 + \\sum_{j=1}^{p} X_j\\beta_j $$\n",
    "\n",
    "In other words, the predicted value $Y$ at an input point $X$ is the sum of a constant ($\\beta_0$) and the product of $p$ coefficients ($\\beta_j$) times the input vector, ($X_j$), where $p$ is the dimension of the input space.\n",
    "\n",
    "The $\\beta_j$ are unknown parameters or coefficients (produced by the linear modeling process) and the variables $X_j$ are the quantiative inputs, numeric or \"dummy\" coding of qualitative variables, or interactions between variables (e.g. $X_3 = X_2 * X_1$).\n",
    "\n",
    "The most popular estimation method for finding the coefficients, $\\beta$, is known as *least squares* in which the coefficients are picked to minimize the residual sum of squares. \n",
    "\n",
    "How do we minimize the residual sum of squares? Denote by $X$ the $N Ã— (p + 1)$ matrix with each row an input vector (with a 1 in the first position), and similarly let $y$ be the N-vector of outputs in the training set. Then we can write the residual sum-of-squares as\n",
    "$$ RSS(\\beta) = (y-X\\beta)^T(y-X\\beta)$$\n",
    "\n",
    "With some fancy math we can get to:\n",
    "$$ \\hat{\\beta} = (X^TX)^{-1}X^Ty $$ \n",
    "\n",
    "The predicted values at an input vector $x_0$ are given by \n",
    "$$\\hat{f}(x_0) = (1 : x_0)^T\\hat{\\beta}$$\n",
    "\n",
    "The fitted values at the training inputs are \n",
    "$$ \\hat{y} = X\\hat{\\beta} $$.\n",
    "\n",
    "It might happen that the columns of $X$ are not linearly independent, so that $X$ is not of full rank. This would occur, for example, if two of the inputs were perfectly correlated, (e.g., $x_2 = 3x_1$). Then $X^T X$ is singular and the least squares coefficients $\\hat{\\beta}$ are not uniquely defined. However, the fitted values $\\hat{y} = X\\hat{\\beta}$ are still the projection of $y$ onto the column space of $X$; there is just more than one way to express that projection in terms of the column vectors of $X$. \n",
    "\n",
    "The non-full-rank case occurs most often when one or more qualitative inputs are coded in a redundant fashion. There is usually a natural way to resolve the non-unique representation, by recoding and/or dropping redundant columns in $X$. Most regression software packages detect these redundancies and automatically implement some strategy for removing them. Rank deficiencies can also occur in signal and image analysis, where the number of inputs p can exceed the number of training cases N. In this case, the features are typically reduced by filtering or else the fitting is controlled by regularization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
